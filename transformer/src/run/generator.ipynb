{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894defa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint found: ../checkpoints/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Setup imports and paths\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from modelling.model import TransformerModel\n",
    "\n",
    "# Check for saved model\n",
    "CHECKPOINT_PATH = '../checkpoints/shakespear_model.pt'\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"✓ Checkpoint found: {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(f\"✗ No checkpoint found at {CHECKPOINT_PATH}\")\n",
    "    print(\"  Run 'python -m run.main' first to train the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d883a815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config:\n",
      "  vocab_size: 65\n",
      "  d_model: 128\n",
      "  n_heads: 4\n",
      "  num_encoder_layers: 2\n",
      "  num_decoder_layers: 2\n",
      "  dim_feedforward: 512\n",
      "  dropout: 0.1\n",
      "  block_size: 128\n",
      "\n",
      "Vocab size: 65\n",
      "\n",
      "✓ Model loaded from epoch 3\n",
      "  Train loss: 0.0140\n",
      "  Val loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "\n",
    "# Get config from checkpoint\n",
    "config = checkpoint['config']\n",
    "print(\"Model Config:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Recreate tokenizer\n",
    "class CharTokenizer:\n",
    "    def __init__(self, char_to_idx):\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "        self.vocab_size = len(char_to_idx)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[ch] for ch in text if ch in self.char_to_idx]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.idx_to_char[i] for i in ids if i in self.idx_to_char])\n",
    "\n",
    "tokenizer = CharTokenizer(checkpoint['tokenizer_char_to_idx'])\n",
    "print(f\"\\nVocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Create model\n",
    "model = TransformerModel(\n",
    "    vocab_size=config['vocab_size'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    dim_feedforward=config['dim_feedforward'],\n",
    "    dropout=config['dropout'],\n",
    "    max_len=config['block_size'] + 100\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n✓ Model loaded from epoch {checkpoint['epoch']}\")\n",
    "print(f\"  Train loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"  Val loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae1ce4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generation function ready (fixed!)\n"
     ]
    }
   ],
   "source": [
    "# Text generation function\n",
    "def generate_text(prompt, max_length=200, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text from the model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_length: How many characters to generate\n",
    "        temperature: Higher = more creative, Lower = more conservative\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Keep track of ALL generated tokens (not just context window)\n",
    "    all_tokens = list(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass - use only last block_size tokens for model\n",
    "            context = input_tensor[:, -config['block_size']:] if input_tensor.size(1) > config['block_size'] else input_tensor\n",
    "            output = model(context, context)\n",
    "            \n",
    "            # Get last token logits and apply temperature\n",
    "            logits = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            next_token_id = next_token.item()\n",
    "            \n",
    "            # Add to full sequence AND input tensor\n",
    "            all_tokens.append(next_token_id)\n",
    "            input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Return ALL tokens including prompt\n",
    "    return tokenizer.decode(all_tokens)\n",
    "\n",
    "print(\"✓ Generation function ready (fixed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ac3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: 'ROMEO:\\n'\n",
      "============================================================\n",
      "ROMEO:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Prompt: 'JULIET:\\n'\n",
      "============================================================\n",
      "JULIET:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Prompt: 'First Citizen:\\n'\n",
      "============================================================\n",
      "First Citizen:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Prompt: 'To be or not to be'\n",
      "============================================================\n",
      "To be or not to beeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeees a core  the hear\n",
      "wee oar the fore this jespetter bed, sir,\n",
      "We speak the are dist from a him give to yet;\n",
      "And my but never thou son to neving that make the were hy the shall bring.\n",
      "\n",
      "KING RIC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate samples with different prompts\n",
    "prompts = [\n",
    "    \"ROMEO:\\n\",\n",
    "    \"JULIET:\\n\", \n",
    "    \"First Citizen:\\n\",\n",
    "    \"To be or not to be\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prompt: {prompt!r}\")\n",
    "    print(\"=\"*60)\n",
    "    text = generate_text(prompt, max_length=300, temperature=0.8)\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5fd57d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAMLET:\n",
      "What dreams may comeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee heaven\n",
      "And it see the sees and and the him the peak\n",
      "And me pring the heaves the live to one son of this like all speak the not for the parce this conss\n",
      "Where him not the the him the conce of the cannot the come and can me this a good son the the to had my plood,\n",
      "And of the peak a the peak and of the cannot the be the king the shall be a distering and that is the side;\n",
      "The see a should then the man a good,\n",
      "And me so she to be heave the shall the could\n",
      "From the some the this we prong of him.\n",
      "\n",
      "KING RICHARD III:\n",
      "The which the would have that I will the face many.\n",
      "\n",
      "CLIFFORD:\n",
      "And and perch of the he\n"
     ]
    }
   ],
   "source": [
    "prompt = \"HAMLET:\\nWhat dreams may come\"\n",
    "temperature = 0.5  \n",
    "\n",
    "print(generate_text(prompt, max_length=700, temperature=temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295857b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
