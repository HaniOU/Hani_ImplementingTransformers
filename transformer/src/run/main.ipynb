{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c305d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hani/Library/Caches/pypoetry/virtualenvs/transformer-6K1zscZO-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Imports and Setup\n",
    "# ==============================================================================\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Optional\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from modelling.model import TransformerModel\n",
    "from data_utils import clean_text_pair\n",
    "\n",
    "# Device configuration (supports CUDA, MPS for Apple Silicon, or CPU)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93b25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wmt17' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WMT17 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 5906184/5906184 [00:01<00:00, 3771059.50 examples/s]\n",
      "Generating validation split: 100%|██████████| 2999/2999 [00:00<00:00, 1114838.05 examples/s]\n",
      "Generating test split: 100%|██████████| 3004/3004 [00:00<00:00, 1242940.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. Load WMT17 German-English Dataset\n",
    "# ==============================================================================\n",
    "print(\"Loading WMT17 dataset...\")\n",
    "dataset = load_dataset(\"wmt17\", \"de-en\", trust_remote_code=True)\n",
    "print(f\"✓ Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2068d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning test data:  54%|█████▍    | 108/200 [00:00<00:00, 32356.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cleaned 100 test pairs\n",
      "\n",
      "Example pair:\n",
      "  German: 28-jähriger koch in san francisco mall tot aufgefunden\n",
      "  English: 28-year-old chef found dead at san francisco mall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2b. Clean Test Data for Evaluation\n",
    "# ==============================================================================\n",
    "test_data = dataset['test']['translation']\n",
    "NUM_TEST_SAMPLES = 100  # Use subset for faster evaluation\n",
    "\n",
    "cleaned_pairs = []\n",
    "for item in tqdm(test_data[:NUM_TEST_SAMPLES * 2], desc=\"Cleaning test data\"):\n",
    "    result = clean_text_pair(item['de'], item['en'])\n",
    "    if result is not None:\n",
    "        cleaned_pairs.append(result)\n",
    "    if len(cleaned_pairs) >= NUM_TEST_SAMPLES:\n",
    "        break\n",
    "\n",
    "print(f\"✓ Cleaned {len(cleaned_pairs)} test pairs\")\n",
    "print(f\"\\nExample pair:\")\n",
    "print(f\"  German:  {cleaned_pairs[0][0]}\")\n",
    "print(f\"  English: {cleaned_pairs[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a582bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer on WMT17 data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing corpus: 100%|██████████| 50000/50000 [00:00<00:00, 6202022.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "✓ Tokenizer trained and saved\n",
      "\n",
      "✓ YOUR GPT2BPETokenizer ready\n",
      "  Vocab size: 32000\n",
      "  PAD=1, BOS=2, EOS=3\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Train Tokenizer on WMT17 Data (if not already trained)\n",
    "# ==============================================================================\n",
    "from tokenizer import GPT2BPETokenizer\n",
    "import os\n",
    "\n",
    "TOKENIZER_ARTIFACTS_DIR = '../tokenizer_artifacts'\n",
    "\n",
    "# Check if tokenizer already exists\n",
    "if os.path.exists(os.path.join(TOKENIZER_ARTIFACTS_DIR, 'vocab.json')):\n",
    "    print(\"✓ Tokenizer already trained, loading from artifacts...\")\n",
    "    from transformers import GPT2Tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "        TOKENIZER_ARTIFACTS_DIR,\n",
    "        unk_token=\"<unk>\", pad_token=\"<pad>\", bos_token=\"<s>\", eos_token=\"</s>\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Training tokenizer on WMT17 data...\")\n",
    "    # Collect training corpus (use subset for faster training)\n",
    "    train_data = dataset['train']['translation']\n",
    "    corpus = []\n",
    "    for item in tqdm(train_data[:50000], desc=\"Preparing corpus\"):\n",
    "        corpus.append(item['de'])\n",
    "        corpus.append(item['en'])\n",
    "    \n",
    "    # Train YOUR GPT2BPETokenizer\n",
    "    gpt2_tokenizer = GPT2BPETokenizer(\n",
    "        corpus=corpus,\n",
    "        vocab_size=32000,\n",
    "        special_tokens=[\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"],\n",
    "        artifact_dir=TOKENIZER_ARTIFACTS_DIR\n",
    "    )\n",
    "    tokenizer = gpt2_tokenizer.tokenizer\n",
    "    print(\"✓ Tokenizer trained and saved\")\n",
    "\n",
    "# Special token IDs\n",
    "PAD_ID = tokenizer.pad_token_id\n",
    "BOS_ID = tokenizer.bos_token_id  \n",
    "EOS_ID = tokenizer.eos_token_id\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "print(f\"\\n✓ YOUR GPT2BPETokenizer ready\")\n",
    "print(f\"  Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"  PAD={PAD_ID}, BOS={BOS_ID}, EOS={EOS_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2ec420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No trained model found at ../checkpoints/translation_model.pt\n",
      "  Model initialized with random weights - train it first!\n",
      "✓ Model ready on mps\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Create or Load Trained Translation Model\n",
    "# ==============================================================================\n",
    "CHECKPOINT_PATH = '../checkpoints/translation_model.pt'\n",
    "MAX_SEQ_LEN = 256  # Maximum sequence length for both source and target\n",
    "\n",
    "# Model configuration for translation\n",
    "MODEL_CONFIG = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'd_model': 256,\n",
    "    'n_heads': 8,\n",
    "    'num_encoder_layers': 3,\n",
    "    'num_decoder_layers': 3,\n",
    "    'dim_feedforward': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': MAX_SEQ_LEN  # Increased to handle longer sequences\n",
    "}\n",
    "\n",
    "# Create model with YOUR custom transformer implementation\n",
    "model = TransformerModel(\n",
    "    vocab_size=MODEL_CONFIG['vocab_size'],\n",
    "    d_model=MODEL_CONFIG['d_model'],\n",
    "    n_heads=MODEL_CONFIG['n_heads'],\n",
    "    num_encoder_layers=MODEL_CONFIG['num_encoder_layers'],\n",
    "    num_decoder_layers=MODEL_CONFIG['num_decoder_layers'],\n",
    "    dim_feedforward=MODEL_CONFIG['dim_feedforward'],\n",
    "    dropout=MODEL_CONFIG['dropout'],\n",
    "    max_len=MODEL_CONFIG['max_len']\n",
    ")\n",
    "\n",
    "# Load trained weights if available\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Loaded trained translation model from {CHECKPOINT_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠ No trained model found at {CHECKPOINT_PATH}\")\n",
    "    print(\"  Model initialized with random weights - train it first!\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f\"✓ Model ready on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b98cc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning training data:  56%|█████▋    | 56250/100000 [00:01<00:01, 43634.48it/s]\n",
      "Cleaning validation data: 100%|██████████| 1000/1000 [00:00<00:00, 52377.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training pairs: 50000\n",
      "✓ Validation pairs: 942\n",
      "✓ Train batches: 1563\n",
      "✓ Val batches: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4b. Prepare Training Data\n",
    "# ==============================================================================\n",
    "from dataset import TranslationDataset\n",
    "from data_utils import collate_batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clean training data (use subset for faster training - increase for better results)\n",
    "NUM_TRAIN_SAMPLES = 50000  # Increase this for better model quality\n",
    "train_data = dataset['train']['translation']\n",
    "\n",
    "train_pairs = []\n",
    "for item in tqdm(train_data[:NUM_TRAIN_SAMPLES * 2], desc=\"Cleaning training data\"):\n",
    "    result = clean_text_pair(item['de'], item['en'])\n",
    "    if result is not None:\n",
    "        train_pairs.append(result)\n",
    "    if len(train_pairs) >= NUM_TRAIN_SAMPLES:\n",
    "        break\n",
    "\n",
    "# Clean validation data\n",
    "val_data = dataset['validation']['translation']\n",
    "val_pairs = []\n",
    "for item in tqdm(val_data[:1000], desc=\"Cleaning validation data\"):\n",
    "    result = clean_text_pair(item['de'], item['en'])\n",
    "    if result is not None:\n",
    "        val_pairs.append(result)\n",
    "\n",
    "print(f\"✓ Training pairs: {len(train_pairs)}\")\n",
    "print(f\"✓ Validation pairs: {len(val_pairs)}\")\n",
    "\n",
    "# Create datasets using YOUR TranslationDataset class\n",
    "train_dataset = TranslationDataset(train_pairs, tokenizer)\n",
    "val_dataset = TranslationDataset(val_pairs, tokenizer)\n",
    "\n",
    "# Create dataloaders with YOUR collate_batch function\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return collate_batch(batch, pad_idx=PAD_ID, bos_idx=BOS_ID, eos_idx=EOS_ID)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daaadd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer initialized\n",
      "  Epochs: 5\n",
      "  Learning rate: 0.0001\n",
      "  Warmup steps: 1000\n",
      "  d_model: 256\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4c. Train the Translation Model\n",
    "# ==============================================================================\n",
    "from modelling.trainer import Trainer\n",
    "from modelling.scheduler import get_scheduler\n",
    "from modelling.loss import get_loss_function\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "WARMUP_STEPS = 1000\n",
    "GRAD_CLIP = 1.0\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Initialize scheduler using YOUR NoamScheduler\n",
    "scheduler = get_scheduler(\n",
    "    optimizer, \n",
    "    scheduler_type='noam',\n",
    "    d_model=MODEL_CONFIG['d_model'],\n",
    "    warmup_steps=WARMUP_STEPS\n",
    ")\n",
    "\n",
    "# Initialize loss function using YOUR loss function (with label smoothing)\n",
    "criterion = get_loss_function('label_smoothing', smoothing=0.1, ignore_index=PAD_ID)\n",
    "\n",
    "# Create trainer using YOUR Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=DEVICE,\n",
    "    grad_clip=GRAD_CLIP\n",
    ")\n",
    "\n",
    "print(f\"✓ Trainer initialized\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "print(f\"  d_model: {MODEL_CONFIG['d_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c062231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1563/1563 [12:14<00:00,  2.13it/s, loss=3.7030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.3607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 30/30 [00:04<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.9755\n",
      "  ✓ Saved best model (val_loss: 4.9755)\n",
      "Learning Rate: 0.001580\n",
      "\n",
      "==================================================\n",
      "Epoch 2/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1563/1563 [32:58<00:00,  1.27s/it, loss=3.5654]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.6579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 30/30 [00:05<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.8349\n",
      "  ✓ Saved best model (val_loss: 4.8349)\n",
      "Learning Rate: 0.001118\n",
      "\n",
      "==================================================\n",
      "Epoch 3/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   1%|▏         | 22/1563 [00:12<14:25,  1.78it/s, loss=3.5586]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4d. Train using YOUR Trainer.train() method\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train with YOUR trainer's built-in train method\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m best_val_loss = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_extra\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_CONFIG\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Training complete! Best val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/semester1/Hani_ImplementingTransformers/transformer/src/modelling/trainer.py:201\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader, num_epochs, save_path, save_extra, on_epoch_end)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/semester1/Hani_ImplementingTransformers/transformer/src/modelling/trainer.py:85\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self, dataloader, epoch)\u001b[39m\n\u001b[32m     81\u001b[39m output = \u001b[38;5;28mself\u001b[39m.model(src, tgt_input, src_mask, tgt_mask)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Reshape for loss calculation: (batch * seq_len, vocab_size) and (batch * seq_len,)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/transformer-6K1zscZO-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/transformer-6K1zscZO-py3.13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/semester1/Hani_ImplementingTransformers/transformer/src/modelling/loss.py:56\u001b[39m, in \u001b[36mLabelSmoothingCrossEntropy.forward\u001b[39m\u001b[34m(self, pred, target)\u001b[39m\n\u001b[32m     53\u001b[39m     true_dist[mask] = \u001b[32m0\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Calculate KL divergence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m pred_log_prob = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m loss = -(true_dist * pred_log_prob).sum(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Apply mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/transformer-6K1zscZO-py3.13/lib/python3.13/site-packages/torch/nn/functional.py:2213\u001b[39m, in \u001b[36mlog_softmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2209\u001b[39m         ret = y_soft\n\u001b[32m   2210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[32m-> \u001b[39m\u001b[32m2213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_softmax\u001b[39m(\n\u001b[32m   2214\u001b[39m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[32m   2215\u001b[39m     dim: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2216\u001b[39m     _stacklevel: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m,\n\u001b[32m   2217\u001b[39m     dtype: Optional[DType] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2218\u001b[39m ) -> Tensor:\n\u001b[32m   2219\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Apply a softmax followed by a logarithm.\u001b[39;00m\n\u001b[32m   2220\u001b[39m \n\u001b[32m   2221\u001b[39m \u001b[33;03m    While mathematically equivalent to log(softmax(x)), doing these two\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2232\u001b[39m \u001b[33;03m          is performed. This is useful for preventing data type overflows. Default: None.\u001b[39;00m\n\u001b[32m   2233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4d. Train using YOUR Trainer.train() method\n",
    "# ==============================================================================\n",
    "\n",
    "# Train with YOUR trainer's built-in train method\n",
    "best_val_loss = trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    num_epochs=EPOCHS,\n",
    "    save_path=CHECKPOINT_PATH,\n",
    "    save_extra={'config': MODEL_CONFIG}\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training complete! Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee11ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded best model from epoch 2\n",
      "  Val loss: 4.8349\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4e. Reload Best Model for Evaluation\n",
    "# ==============================================================================\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "print(f\"✓ Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"  Val loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98bea56",
   "metadata": {},
   "source": [
    "## Exercise 1: Greedy Decoding for Translation\n",
    "\n",
    "Implement autoregressive generation:\n",
    "1. Encode the source sentence with the encoder\n",
    "2. Start decoder with BOS token\n",
    "3. At each step, take the token with highest probability (greedy)\n",
    "4. Stop when EOS is generated or max length is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9105b46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Greedy decoding function defined\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. Greedy Decoding Function for Translation\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_decode(\n",
    "    model: TransformerModel,\n",
    "    src_ids: List[int],\n",
    "    bos_id: int,\n",
    "    eos_id: int,\n",
    "    max_length: int = 100,\n",
    "    device: str = 'cpu'\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Generate a translation using greedy decoding.\n",
    "    \n",
    "    Autoregressive generation procedure:\n",
    "    1. Encode the source sentence using the encoder\n",
    "    2. Initialize decoder with BOS token\n",
    "    3. Generate tokens one at a time, selecting highest probability (greedy)\n",
    "    4. Stop when EOS is generated or max length is reached\n",
    "    \n",
    "    Args:\n",
    "        model: The trained transformer model\n",
    "        src_ids: Source sentence token IDs\n",
    "        bos_id: Beginning of sequence token ID\n",
    "        eos_id: End of sequence token ID\n",
    "        max_length: Maximum generation length\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        List of generated token IDs\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare source tensor\n",
    "    src = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "    src_mask = torch.ones(1, len(src_ids), dtype=torch.long, device=device)\n",
    "    \n",
    "    # Step 1: Encode source sequence once\n",
    "    encoder_output = model.encode(src, src_mask)\n",
    "    \n",
    "    # Step 2: Initialize decoder input with BOS token\n",
    "    decoder_input = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "    generated_ids = []\n",
    "    \n",
    "    # Step 3 & 4: Generate tokens autoregressively\n",
    "    for _ in range(max_length):\n",
    "        # Create decoder mask\n",
    "        tgt_mask = torch.ones(1, decoder_input.size(1), dtype=torch.long, device=device)\n",
    "        \n",
    "        # Decode: get output from decoder\n",
    "        decoder_output = model.decode(decoder_input, encoder_output, tgt_mask, src_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = model.output_projection(decoder_output)\n",
    "        \n",
    "        # Get the last token's logits and find the most probable next token (GREEDY)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits).item()\n",
    "        \n",
    "        # Append to generated sequence\n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        # Stop if EOS is generated\n",
    "        if next_token_id == eos_id:\n",
    "            break\n",
    "        \n",
    "        # Update decoder input for next iteration\n",
    "        decoder_input = torch.cat([\n",
    "            decoder_input,\n",
    "            torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "        ], dim=1)\n",
    "    \n",
    "    return generated_ids\n",
    "\n",
    "\n",
    "def translate(\n",
    "    model: TransformerModel,\n",
    "    source_text: str,\n",
    "    tokenizer,\n",
    "    max_length: int = 100,\n",
    "    device: str = 'cpu'\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translate a source sentence (German) to target language (English).\n",
    "    \"\"\"\n",
    "    # Encode source text\n",
    "    src_ids = tokenizer.encode(source_text)\n",
    "    \n",
    "    # Generate translation using greedy decoding\n",
    "    generated_ids = greedy_decode(\n",
    "        model, src_ids, \n",
    "        bos_id=tokenizer.bos_token_id,\n",
    "        eos_id=tokenizer.eos_token_id,\n",
    "        max_length=max_length, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Decode to text\n",
    "    translation = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return translation\n",
    "\n",
    "print(\"✓ Greedy decoding function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b7be3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing translation on sample sentences:\n",
      "\n",
      "================================================================================\n",
      "Example 1:\n",
      "  Source (DE):     28-jähriger koch in san francisco mall tot aufgefunden\n",
      "  Reference (EN):  28-year-old chef found dead at san francisco mall\n",
      "  Translation:     thecommissionhasnottobeaveryimportantway.\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2:\n",
      "  Source (DE):     ein 28-jähriger koch, der vor kurzem nach san francisco gezogen ist, wurde im treppenhaus eines örtlichen einkaufzentrums tot aufgefunden.\n",
      "  Reference (EN):  a 28-year-old chef who had recently moved to san francisco was found dead in the stairwell of a local mall this week.\n",
      "  Translation:     thecommissionhastobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeaveryimportantway.\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3:\n",
      "  Source (DE):     der bruder des opfers sagte aus, dass er sich niemanden vorstellen kann, der ihm schaden wollen würde, endlich ging es bei ihm wieder bergauf.\n",
      "  Reference (EN):  but the victims brother says he cant think of anyone who would want to hurt him, saying, things were finally going well for him.\n",
      "  Translation:     thecommissionhastobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeaveryimportantway.\n",
      "--------------------------------------------------------------------------------\n",
      "Example 4:\n",
      "  Source (DE):     der am mittwoch morgen in der westfield mall gefundene leichnam wurde als der 28 jahre alte frank galicia aus san francisco identifiziert, teilte die gerichtsmedizinische abteilung in san francisco mit.\n",
      "  Reference (EN):  the body found at the westfield mall wednesday morning was identified as 28-year-old san francisco resident frank galicia, the san francisco medical examiners office said.\n",
      "  Translation:     thecommissionhastobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobeabletobe\n",
      "--------------------------------------------------------------------------------\n",
      "Example 5:\n",
      "  Source (DE):     das san francisco police department sagte, dass der tod als mord eingestuft wurde und die ermittlungen am laufen sind.\n",
      "  Reference (EN):  the san francisco police department said the death was ruled a homicide and an investigation is ongoing.\n",
      "  Translation:     thecommissionhasnottobeabletobeabletobeabletobeabletobeabletobeabletobeaveryimportantway.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. Test Translation on a Few Examples\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Testing translation on sample sentences:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, (src, ref) in enumerate(cleaned_pairs[:5]):\n",
    "    translation = translate(model, src, tokenizer, max_length=100, device=DEVICE)\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Source (DE):     {src}\")\n",
    "    print(f\"  Reference (EN):  {ref}\")\n",
    "    print(f\"  Translation:     {translation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a734f",
   "metadata": {},
   "source": [
    "## Exercise 2 & 3: Generate Translations and Compute BLEU Score\n",
    "\n",
    "Generate translations for the test set and evaluate using BLEU score from HuggingFace `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Generate Translations for Test Set\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Generating translations for test set...\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for src, ref in tqdm(cleaned_pairs, desc=\"Translating\"):\n",
    "    # Generate translation\n",
    "    translation = translate(model, src, tokenizer, max_length=100, device=DEVICE)\n",
    "    predictions.append(translation)\n",
    "    references.append([ref])  # BLEU expects list of references\n",
    "\n",
    "print(f\"✓ Generated {len(predictions)} translations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e49c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Compute BLEU Score\n",
    "# ==============================================================================\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Compute BLEU score\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BLEU Score Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  BLEU Score:       {results['bleu']:.4f}\")\n",
    "print(f\"  Precisions:       {[f'{p:.4f}' for p in results['precisions']]}\")\n",
    "print(f\"  Brevity Penalty:  {results['brevity_penalty']:.4f}\")\n",
    "print(f\"  Length Ratio:     {results['length_ratio']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea196c1f",
   "metadata": {},
   "source": [
    "## Exercise 4: Analyze Translation Quality\n",
    "\n",
    "Evaluate some translations and identify common errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 9. Analyze Individual Translations\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Detailed Analysis of Sample Translations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute individual BLEU scores\n",
    "for i in range(min(10, len(predictions))):\n",
    "    src, ref = cleaned_pairs[i]\n",
    "    pred = predictions[i]\n",
    "    \n",
    "    # Individual BLEU\n",
    "    individual_bleu = bleu.compute(predictions=[pred], references=[[ref]])\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} (BLEU: {individual_bleu['bleu']:.4f}) ---\")\n",
    "    print(f\"Source (DE):    {src}\")\n",
    "    print(f\"Reference (EN): {ref}\")\n",
    "    print(f\"Prediction:     {pred}\")\n",
    "    \n",
    "    # Simple error analysis\n",
    "    ref_words = set(ref.lower().split())\n",
    "    pred_words = set(pred.lower().split())\n",
    "    \n",
    "    missing = ref_words - pred_words\n",
    "    extra = pred_words - ref_words\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"Missing words:  {', '.join(list(missing)[:5])}\")\n",
    "    if extra:\n",
    "        print(f\"Extra words:    {', '.join(list(extra)[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 10. Error Pattern Summary\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Common Error Patterns Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze error patterns\n",
    "repetition_count = 0\n",
    "too_short = 0\n",
    "too_long = 0\n",
    "empty_translations = 0\n",
    "\n",
    "for pred, (src, ref) in zip(predictions, cleaned_pairs):\n",
    "    # Check for repetitions\n",
    "    words = pred.split()\n",
    "    if len(words) > 2:\n",
    "        for j in range(len(words) - 2):\n",
    "            if words[j] == words[j+1] == words[j+2]:\n",
    "                repetition_count += 1\n",
    "                break\n",
    "    \n",
    "    # Length analysis\n",
    "    ref_len = len(ref.split())\n",
    "    pred_len = len(pred.split())\n",
    "    \n",
    "    if pred_len == 0:\n",
    "        empty_translations += 1\n",
    "    elif pred_len < ref_len * 0.5:\n",
    "        too_short += 1\n",
    "    elif pred_len > ref_len * 1.5:\n",
    "        too_long += 1\n",
    "\n",
    "print(f\"  Translations with repetitions: {repetition_count}/{len(predictions)}\")\n",
    "print(f\"  Too short translations:        {too_short}/{len(predictions)}\")\n",
    "print(f\"  Too long translations:         {too_long}/{len(predictions)}\")\n",
    "print(f\"  Empty translations:            {empty_translations}/{len(predictions)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ Practical 10 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
